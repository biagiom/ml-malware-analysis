"""
    This file contains an implementation of the Naive Bayes classifier machine learning algoritm and
    it has been designed expecially for problems that involves text classification but it can be also
    extended to other generic classification problems.
    It has been also trained using the SMS Spam Collection data set available at the following link:
    https://archive.ics.uci.edu/ml/datasets/sms+spam+collection.

    Author: Biagio Montaruli <biagio-mkr@libero.it>
"""

import types
from math import ceil
import random


class NaiveBayesText(object):
    """
    This class implements the Naive Bayes ML algorithm designed expecially for text classification.
    Naive Bayes classifiers are a family of classifiers based on the Bayes theorem and they are very fast
    to train and to predict in fact are especially designed for very large datasets.
    The following implementation supports two different kinds of implementation:
    Bernoulli and Multinomial Naive Bayes.
    """

    def __init__(self, debug_mode=False, perc=2/3, rand_seed=0):
        """ Naive Bayes classifier for text classification.

            Parameters
            ------------
            parse_func : function
                function to use in order to parse documents and build the dataset
            param_list : tuple
                list of parameters for the parse_func.
            debug_mode : bool (default False)
                Boolean flag to set the debug mode.
            perc: float (default 2/3)
                The ratio between the number of samples in the training set and the one in the dataset.
            random_split : bool (default True)
                Set random split mode: the dataset will be split into training set and test set randomly.

            Attributes
            -----------
            _debug : bool
                Debug flag.
            _perc : float
                Variable to store the parameter perc.
            vocabulary : list
                list to store all the words found while parsing each document
            Pc : dictionary
                each element represents the probability related to a specific class label (c)
                given the docs in the trainig set.
                Pc_i = (# of docs belonging to class 'c') / (# of docs in the training set)
            Pw_c : dictionary
                each element represents the probability related to the word 'w' given the
                class label 'c' and the docs in the trainig set
        """
        self._debug = debug_mode
        self._perc = perc
        # set random seed
        random.seed(rand_seed)

        self.vocabulary = None

        self.Pc = None
        self.Pw_c = None

    def split_dataset(self, dataset, random_split=False):
        """ Load the dataset using the 'parse_func' parameter and split it into training and test set.

            Parameters
            ------------
            dataset : set (dictionary) of dictionaries, shape = [n_samples, n_features]

            Returns
            -------
            training_set : array-like object (list of dictionaries)
                It contains the samples (docs) that will be used to train the Naive Bayes algorithm
            training_class : array-like object (list of dictionaries)
                It contains the class labels for the samples in the training set
            test_set : array-like object (list of dictionaries)
                It contains the samples (docs) that will be used to evaluate the performances of the
                Naive Bayes algorithm
            test_class : array-like object (list of dictionaries)
                It contains the class labels for the samples in the test set
            classes : dictionary
        """

        # 'training_set' and 'test_set' are list of dictionaries having the following structure:
        # [{"target": <class_label>, "features": <dict of words>}, {...}, ...]
        training_set = []
        test_set = []
        # 'training_class' and 'test_class' are list of class labels:
        # [<label-doc1>, ..., <label-docN>]
        training_class = []
        test_class = []

        # dictionary of class labels having the following structure:
        # classes = {'<class label>: <# of samples belonging to the class>}
        # e.g: classes = {'ham': 0, 'spam': 0}
        classes = None

        if dataset is not None:

            self.vocabulary = []

            # number of samples to add in the training set
            n = ceil(len(dataset) * self._perc)
            print("Number of docs in the training set: {}".format(n))
            print("Number of docs in the test set: {}".format(len(dataset) - n))

            # TODO: classes = dict.fromkeys(set([dataset[obj]["target"] for obj in dataset]), 0)
            classes = {c: 0 for c in set([dataset[obj]["target"] for obj in dataset])}

            # If random_split is set to True, we copy n docs from the dataset to the
            # training set. In addition, we build the test set as the difference between the
            # dataset and the training set.
            if random_split is True:
                training_set_apps = random.sample(dataset.keys(), n)
                for (i, app) in enumerate(training_set_apps):
                    training_set.append(dataset[app]["features"])
                    training_class.append(dataset[app]["target"])

                    if training_class[i] in classes:
                        classes[training_class[i]] += 1

                    for feature in training_set[i]:
                        if feature not in self.vocabulary:
                            self.vocabulary.append(feature)

                test_set_apps = list(set(dataset.keys()) - set(training_set_apps))
                for app in test_set_apps:
                    test_set.append(dataset[app]["features"])
                    test_class.append(dataset[app]["target"])
            else:
                for (i, x) in enumerate(dataset):
                    if i in range(n):
                        training_set.append(dataset[x]["features"])
                        training_class.append(dataset[x]["target"])

                        if training_class[i] in classes:
                            classes[training_class[i]] += 1

                        for feature in training_set[i]:
                            if feature not in self.vocabulary:
                                self.vocabulary.append(feature)
                    else:
                        test_set.append(dataset[x]["features"])
                        test_class.append(dataset[x]["target"])

            if self._debug is True:
                # print info about the vocabulary
                print("Vocabulary (built using the training set):")
                for i in range(len(self.vocabulary)):
                    if i == (len(self.vocabulary) - 1):
                        print(self.vocabulary[i])
                    else:
                        print(self.vocabulary[i], end=", ")
                print('{:-<50}'.format(""))

                print("Number of docs per classes:")
                for c in classes:
                    print("{cl}: {count}".format(cl=c, count=classes[c]))
                print('{:-<50}'.format(""))

                print("Training set ({num} docs)".format(num=n))
                # print all the samples in the training set
                for c, sms in zip(training_class, training_set):
                    print("{} -> {}".format(c, sms))
                print('{:-<50}'.format(""))

                print("Test set ({num} samples)".format(num=(len(dataset) - n)))
                # print all the samples in the test set
                for c, sms in zip(test_class, test_set):
                    print("{} -> {}".format(c, sms))
                print('{:-<50}'.format(""))
        else:
            print("NaiveBayesText Error: Failed to build the dataset")

        return training_set, training_class, test_set, test_class, classes


    def fit(self, training_set, training_class, classes, mode="bernoulli"):
        """ Fit the Naive Bayes algorithm using the training set.

            Parameters
            ----------
            training_set : array-like object (list of dictionaries)
                It contains the samples (docs) used to train the Naive Bayes algorithm
            training_class : array-like object (list of dictionaries)
                It contains the class labels for the samples in the training set
            classes: dictionary
                dictionary of class labels
            mode : string or int (default "bernoulli"/0)
                select the type of Naive Bayes classifier (Bernoulli/Multinomial)

            Returns
            -------
            None
        """
        if (len(training_set) != 0) and (len(training_class) != 0) and \
           (classes is not None) and (len(self.vocabulary) != 0):

            # Pc is a dictionary and each element represents the probability related to a
            # specific class label (c) given the docs in the trainig set
            # Pc_i = (# of docs belonging to class 'c') / (# of docs in the training set)
            self.Pc = {}

            # Pw_c is a dictionary and each element represents the probability related to the
            # word 'w' given the class label 'c' and the docs in the trainig set.
            self.Pw_c = {}

            if mode == "multinomial" or mode == 1:
                # TF_ij is a dictionary and each element represents the total number of times
                # the word 'w_i' occurs in the documents of class 'c_j'.
                TF_ij = {}

            if self._debug is True:
                if mode == "bernoulli" or mode == 0:
                    print("Text classification using Naive Bayes with Bernoulli distribution")
                else:
                    print("Text classification using Naive Bayes with Multinomial distribution")
                print("Total number of docs: {}".format(len(training_set)))

            target = list(classes.keys())
            # for j in range(0, (len(target)-1)):
            for j in range(len(target)):

                T_j = classes[target[j]]
                self.Pc[target[j]] = T_j / len(training_set)

                self.Pw_c[target[j]] = {}

                if self._debug is True:
                    print("Class: {}".format(target[j]))
                    print("Total number of docs in class {c} -> t_j: {tj}".format(c=target[j], tj=T_j))
                    print("P(c_j): {}".format(self.Pc[target[j]]))
                    print('{:-<50}'.format(""))

                if mode == "bernoulli" or mode == 0:

                    for w in self.vocabulary:
                        # T_ij = number of documents in class 'c_j' that contains the word 'w_i'
                        T_ij = 0
                        for idx in range(len(training_set)):
                            if w in training_set[idx] and training_class[idx] == target[j]:
                                T_ij += 1
                        # P(w_i | c_j) = (T_ij + 1) / (T_j + 2)
                        self.Pw_c[target[j]][w] = (T_ij + 1) / (T_j + 2)

                        if self._debug is True:
                            print("Word: {}".format(w))
                            print("Docs of class '{c}' containing '{w}' -> t_ij: {t_ij}"
                                  .format(c=target[j], w=w, t_ij=T_ij))
                            print("P('{w}' | '{c}'): {p}".format(w=w, c=target[j], p=self.Pw_c[target[j]][w]))
                    if self._debug is True:
                        print('{:-<50}'.format(""))

                elif mode == "multinomial" or mode == 1:
                    # TF_j = total number of words in documents of class 'c_j' counting also the duplicates
                    TF_j = 0

                    # for idx in range(0, (len(D) - 1)):
                    # compute TF_j for each document in the current class select in the outer loop
                    for idx in range(len(training_set)):
                        if training_class[idx] == target[j]:
                            words = list(training_set[idx].keys())
                            for w in words:
                                TF_j += training_set[idx][w]

                    TF_ij[target[j]] = {}

                    if self._debug is True:
                        print("Total number of words in docs of class '{c}' -> TF_j: {tf_j}"
                              .format(c=target[j], tf_j=TF_j))

                    for w in self.vocabulary:
                        TF_ij[target[j]][w] = 0

                        # for idx in range(0, (len(D) - 1)):
                        for idx in range(len(training_set)):
                            if training_class[idx] == target[j]:
                                if w in training_set[idx]:
                                    TF_ij[target[j]][w] += training_set[idx][w]
                        self.Pw_c[target[j]][w] = (TF_ij[target[j]][w] + 1) / (TF_j + len(self.vocabulary))

                        if self._debug is True:
                            print("Word: ", w)
                            print("Occurrencies of '{w}' in docs of class '{c}' -> TF_ij: {tf_ij}".format(w=w, c=target[j],  tf_ij=TF_ij[target[j]][w]))
                            print("P(w|c): ", self.Pw_c[target[j]][w])
                    if self._debug is True:
                        print('{:-<50}'.format(""))
        else:
            self.Pc = None
            self.Pw_c = None

    def classify(self, test_set, classes):
        """ Classify samples in the test_set given as parameter.

            Parameters
            ----------
            test_set : array-like object (list of dictionaries)
                It contains the samples (docs) used to evaluate the performances of the
                Naive Bayes algorithm
            classes: dictionary
                dictionary of class labels

            Returns
            -------
            predicted_labels : list
                list of the predicted class labels for the samples in the test set
        """
        predicted_labels = []

        if (len(test_set) != 0) and (len(self.vocabulary) != 0) and \
           (self.Pc is not None) and (self.Pw_c is not None):

            for doc in test_set:
                # dictionary where each element (p_i) represents the probability that
                # the document 'doc' belongs to class c_i.
                P_class = {}
                words = list(doc.keys())

                predicted_labels.append(None)
                P_max = 0
                # Best (most probable) class for the current document
                best_class = None

                # target = list(self.Pc.keys())
                target = list(classes.keys())
                # compute the argmax of P_class
                for t in target:
                    P_class[t] = self.Pc[t]
                    for w in words:
                        if w in self.vocabulary:
                            P_class[t] *= self.Pw_c[t][w]

                    if P_class[t] > P_max:
                        P_max = P_class[t]
                        best_class = t

                # if best_class is None (i.e, P_class[t] -> 0 for all t),
                # assign randomly the target value.
                if best_class is None:
                    best_class = random.sample(list(classes), 1)[0]

                # add the found class to the last item inserted into the doc_class list
                predicted_labels[-1] = best_class

        return predicted_labels

    def evaluate(self, predicted_labels, test_class, classes):
        """ Evaluate the performances computing the accuracy and the confusion matrix

            Parameters
            ----------
            predicted_labels : list
                list of the predicted class labels for the samples in test_set
            test_class : array-like object (list of dictionaries)
                It contains the class labels for the samples in the test set
            classes: dictionary
                dictionary of class labels

            Returns
            -------
            accuracy : float
                accuracy realted to classification of new instances of the test set
            conf_matrix : {array-like}, shape = [n_classes, n_classes]
                confusion matrix
        """
        accuracy = None
        conf_matrix = None

        # if (len(predicted_labels) != 0) and (len(test_class) != 0) and (classes is not None):
        if (len(predicted_labels) == len(test_class)) and (classes is not None):
            # number of misses (classification errors)
            miss = 0

            # confusion matrix
            conf_matrix = {}
            for target in classes:
                conf_matrix[target] = {}
                for c in classes:
                    conf_matrix[target][c] = 0

            if self._debug is True:
                print("Evaluated class | True class")
            for (est_class, true_class) in list(zip(predicted_labels, test_class)):
                if self._debug is True:
                    print("{eval} | {real}".format(eval=est_class, real=true_class))
                if est_class != true_class:
                    miss += 1
                # TODO: conf_matrix[est_class][true_class] += 1
                conf_matrix[true_class][est_class] += 1
                # try:
                #     conf_matrix[true_class][est_class] += 1
                # except TypeError:
                #     print("Error: {eval} | {real}".format(eval=est_class, real=true_class))

            accuracy = 1 - miss / len(predicted_labels)

        else:
            print("NaiveBayesText Error: Failed to compute evaluation parameters")

        return accuracy, conf_matrix

    def roc_evaluation(self, predicted_labels, test_class, classes, class_map=None, convert_mulitclass=False):
        """ Evaluate the performances through ROC analysis computing precision, recall, f1-score and
            the false positive rate

            Parameters
            ----------
            predicted_labels : list
                list of the predicted class labels for the samples in test_set
            test_class : array-like object (list of dictionaries)
                It contains the class labels for the samples in the test set
            classes: dictionary
                dictionary of class labels
            class_map : dictionary
                map dictionary for class labels
            convert_mulitclass : bool
                Boolean flag to perform ROC analysis on a multiclass problem.
                In this case the user must also provide the class map grouping all the classes
                in two sets (positive and negative).

            Returns
            -------
            accuracy : float
                accuracy realted to classification of new instances of the test set
            conf_matrix : {array-like}, shape = [n_classes, n_classes]
                confusion matrix

            Note
            -------
            We assume class value '1 for positive values and class value '0' for negative values.
            If you use different names (e.g.: true/false, ham/spam, ...) please provide a map
            dictionary as input parameter for the ROC analysis (e.g.: {1: "true", 0: false})
        """

        if (len(classes) == 2) or (convert_mulitclass is True):
            TP = 0
            TN = 0
            FP = 0
            FN = 0

            # binary classification
            if (len(classes) == 2):
                if class_map is None:
                    class_map = {0: '0', 1: '1'}

                for (est_class, true_class) in list(zip(predicted_labels, test_class)):
                    # true positive
                    if true_class == class_map[1] and est_class == class_map[1]:
                        TP += 1
                    # false positive
                    elif est_class == class_map[1] and true_class == class_map[0]:
                        FP += 1
                    # false negative
                    elif est_class == class_map[0] and true_class == class_map[1]:
                        FN += 1
                    # true negative
                    else:
                        TN += 1
            # multiclass classification
            elif (convert_mulitclass is True) and (class_map is not None):
                for (est_class, true_class) in list(zip(predicted_labels, test_class)):
                    # true positive
                    if true_class in class_map[1] and est_class in class_map[1]:
                        TP += 1
                    # false positive
                    elif est_class in class_map[1] and true_class in class_map[0]:
                        FP += 1
                    # false negative
                    elif est_class in class_map[0] and true_class in class_map[1]:
                        FN += 1
                    # true negative
                    else:
                        TN += 1

            # compute precision
            if TP + FP == 0:
                precision = 1
            else:
                precision = TP / (TP + FP)

            # compute recall
            if TP + FN == 0:
                recall = 1
            else:
                recall = TP / (TP + FN)

            # compute false positive rate
            if FP + TN == 0:
                fpr = 1
            else:
                fpr = TP / (FP + TN)

            # accuracy evaluated using FP, FN, TP, TN:
            acc = (TP + TN) / (TP + TN + FP + FN)
            f1_score = 2 * (precision * recall) / (precision + recall)

            roc_param = {"TP": TP, "TN": TN, "FP": FP, "FN": FN, "accuracy": acc,
                         "precision": precision, "recall": recall, "fpr": fpr, "f1-score": f1_score}

        else:
            roc_param = {"TP": None, "TN": None, "FP": None, "FN": None, "accuracy": None,
                         "precision": None, "recall": None, "fpr": None, "f1-score": None}

        return roc_param
