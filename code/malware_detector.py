import naive_bayes_text as nb
from util import build_dataset
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
import seaborn as sns; sns.set()

dataset_path = "../drebin/feature_vectors/"
family_labels_path = "../drebin/sha256_family.csv"
feature_list = ['permission', 'call', 'api_call', 'intent']
eta = 1
types = ["bernoulli", "multinomial"]
mode = "bin_class"
rseed = 20
num_samples = 10000

DEBUG = False
num_iter = 4

if __name__ == '__main__':

    nb_classifier = nb.NaiveBayesText(debug_mode=False, perc=2/3, rand_seed=rseed)

    dataset = build_dataset(dataset_path, family_labels_path, feature_list, num_samples,
                            eta, rseed, mode, True, False)

    print("**** Malware Detection ****")

    avg_perf = {t: {"accuracy": 0, "precision": 0, "recall": 0, "f1-score": 0} for t in types}

    for n in range(num_iter):
        print("TEST #{}".format(n+1))

        train_set, train_class, test_set, \
        test_class, classes = nb_classifier.split_dataset(dataset, random_split=True)

        for t in types:
            if t == "bernoulli":
                print("\nUsing the Naive Bayes classifier with Multi-variate Bernoulli distribution")
            else:
                print("\nUsing the Naive Bayes classifier with Multinomial distribution")

            nb_classifier.fit(train_set, train_class, classes, t)

            estimated_class = nb_classifier.classify(test_set, classes)

            if DEBUG is True:
                print("Estimated class labels:")
                print(estimated_class)
                print("Target (True) class labels:")
                print(test_class)

            class_map = {1: "malware", 0: "safe"}

            accuracy, conf_matrix = nb_classifier.evaluate(estimated_class, test_class, classes)
            print("Classes:")
            print(classes)
            print("\nAccuracy: {:.2f}".format(accuracy))
            print("Confusion matrix:")
            print(conf_matrix)

            target_names = list(classes.keys())
            plt.figure()
            if t == "bernoulli":
                plt.title("Confusion matrix TEST #{} - Bernoulli Naive Bayes".format(n+1))
            else:
                plt.title("Confusion matrix TEST #{} - Multinomial Naive Bayes".format(n+1))
            sns.heatmap(conf_matrix, square=True, annot=True, fmt='d', cbar=False,
                        xticklabels=target_names, yticklabels=target_names)
            plt.xlabel('predicted label')
            plt.ylabel('true label')
            plt.draw()
            plt.tight_layout()

            metrics = nb_classifier.classification_metrics(estimated_class, test_class, classes, class_map)

            print("\nPerformance evaluation:")
            print("TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}".format(
                tp=metrics["TP"], tn=metrics["TN"], fp=metrics["FP"], fn=metrics["FN"]))

            print("Precision: {:.2f}".format(metrics["precision"]))
            print("Recall: {:.2f}".format(metrics["recall"]))
            print("Accuracy: {:.2f}".format(metrics["accuracy"]))
            print("F1-score: {:.2f}".format(metrics["f1-score"]))

            avg_perf[t]["accuracy"] += metrics["accuracy"]
            avg_perf[t]["precision"] += metrics["precision"]
            avg_perf[t]["recall"] += metrics["recall"]
            avg_perf[t]["f1-score"] += metrics["f1-score"]

            print("\nClassification report using sklearn:")
            print(classification_report(test_class, estimated_class, target_names=target_names))
            print('{:-<60}'.format(""))

        plt.show()

    print("Average performances over {} tests:".format(num_iter))
    for t in types:
        if t == "bernoulli":
            print("> Multi-variate Bernoulli Naive Bayes")
        else:
            print("> Multinomial Naive Bayes")

        for metric in sorted(avg_perf[t]):
            avg_perf[t][metric] /= num_iter
            print("Average {m}: {value:.2f}".format(m=metric, value=avg_perf[t][metric]))

        print('{:-<60}'.format(""))